% !TeX root = Notes.tex

\documentclass{article}
\usepackage{ctex}
\usepackage[utf8]{inputenc}
\usepackage[top=1in, bottom=1in, left=1.in, right=1.in]{geometry} % adjust default margins
\usepackage{fancyhdr}
\pagestyle{fancy}
\lhead{Yiran Wang}\chead{\reportNumber}\rhead{DayDayUp~}\lfoot{}\cfoot{\thepage}\rfoot{} % setting page header and footer
\setlength{\parindent}{0em}\setlength{\parskip}{1.5ex} % use newline to seperate paragraphs
\usepackage[T1]{fontenc}     % oriented to output, that is, what fonts to use for printing characters % [must use] make ligatures copyable
\usepackage{enumitem} %\setlist[itemize]{noitemsep,nolistsep} % same to use \begin{itemize}[nolistsep,noitemsep]
\usepackage[ampersand]{easylist} % ampersand == & % \begin{easylist}[itemize] %% to make easy list environment
\usepackage{amsmath,amssymb,bm}
\usepackage[colorlinks=true,linkcolor=blue,citecolor=blue,urlcolor=blue,breaklinks=true]{hyperref} % color the hyperlink rather than box it
\usepackage{graphicx}
\usepackage{color}
\usepackage[all]{hypcap} % make the hyperlink to point at the upper border of a figure of table, rather than to point to its caption.
\usepackage{breakcites}
\usepackage{cleveref} \crefname{equation}{Eq.}{Eqs.} \Crefname{equation}{Equation}{Equations} \crefname{figure}{Fig.}{Figs.} \Crefname{figure}{Figure}{Figures} \crefname{table}{Table}{Tables} % allow refer to multiple equations and change cross-reference name
\usepackage{caption}
\usepackage{subfigure}
\usepackage{wasysym} % to insert emotion \smiley
\usepackage{float} 
\usepackage[yyyymmdd,hhmmss]{datetime}\renewcommand{\dateseparator}{-}
\usepackage{placeins}
%\usepackage{setspace} \doublespacing \onehalfspacing % 调整行距
%\usepackage{pdflscape} \horizontal figure
\usepackage{ulem}
\usepackage{mathtools}
\usepackage{longtable}
\urlstyle{same}
%\graphicspath{ {./Test_01/} }
\bibliographystyle{abbrv}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newcommand{\yrSay}[1]{\textcolor{blue}{#1}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\title{LLM Notes}
\author{\mathfrak{Y} \mathfrak{R} }
\date{Updated on \today}

\begin{document}
\maketitle

\section{一些专业名词简单定义}


    
\begin{longtable}{p{3cm}p{5cm}p{9cm}}\toprule
\hline
\hline
中文分词 & Chinese Word Segmentation, CWS & 处理中文文本时，由于词与词之间灭有明显分隔（空格），所以无法直接通过空格来确定词的边界。
                                            其目的是将连续的中文文本切分成有意义的词汇序列。 \\
\hline
子词切分 & Subword Segmentation           &   特别适合处理词汇稀疏的问题，即当遇到罕见词或者未见过的新词时，能够通过已知的子词单位来理解或生成这些词汇。
                                            在处理拼写复杂，合成词多的语言（德语）或预训练语言模型（BERT，GPT）中尤为重要。
                                            常用的方法有Byte Pair Encoding (BPE), WordPiece, Unigram, SentencePiece。\\                                                                 \\
\hline
词性标注 & Part of speech Tagging, POS Tagging & 为文本中的每一个单词分配一个词性标签，如名词动词形容词。 
                                                POS tagging对理解句子结构，进行句法分析，语义角色标注等高级NLP任务至关重要。
                                                计算机可以更好地理解文本的含义，进行信息提取，情感飞，机器翻译。。
                                                通常依赖于机器学习模型，如隐马尔科夫模型(Hidden Markov Model HMM), 条件随机场（COnditional Random Field CRF),或RNN，LSTM等。
                                                通过学习大量的标注数据来预测新句子中每个单词的词性。\\
\hline
文本分类 & Text Classfication & 将给定的文本自动分配到一个或多个预定义的类别中。应用包括但不限于情感分析，垃圾邮件检测，新闻分类，主题识别等。
                                文本分类的关键在于理解文本的含义和上下文，并基于此将文本映射到特定的类别。
                                文本分类的关键在于选择合适的特征表示和分类算法，以及拥有高质量的训练数据。\\
\hline
 实体识别 (又名，命名实体识别) & Named Entity Recognition, NER & 自动识别文本中具有特定意义的实体，并将它们分类为预定的类别，如人名，地点，组织，日期，时间等。
                                            实体识别任务对于信息提取，知识图谱构建，问答系统，内容推荐等应用很重要，它能够帮助系统理解文本中的关键元素及其属性。\\

\hline
关系抽取 & Relation Extraction & 它的目标是从文本识别实体之间的语义关系。这些关系可以是因果关系，拥有关系，亲属关系，地理关系等。对理解文本内容，
                                构建知识图谱，提升机器理解语言的能力具有重要意义。

\hline
文本摘要 & Text Summarization & 目的是生成一段剪辑真确的摘要，来概括原文的主要内容。根据生成的方式不同，文本摘要可以分为两大类： 抽取式摘要(Extractive Summarization)和生成式摘要(Abstractive Summarization)。\\
        &  抽取式摘要  &        通过直接从原文中选取关键句子或短语来组成摘要。优点：信息玩去哪来自原文，因此准确性较高。 缺：由于仅仅是原文中句子的拼接，有时候可能不够流畅。 \\
        &   生成式摘要 &        不仅需要涉及选择文本片段，还需要对这些片段进行重新组织和改写，并生成新的内容。它更具有挑战性，因为需要理解文本的深层含义，并能够以新的方式表达相同的信息。
                                生成式摘要通常需要更复杂的模型，如注意力机制的Seq2Seq.\\
\hline
机器翻译 & Machine Translation （MT） &  \\
\hline
自动问答 & Automatic Question Answering, QA & 可大致分为三类： 检索式回答（Retrieval-based QA），通过搜索引擎等方式从大量文本中检索答案；
                                知识库问答（knowledge-based QA），通过结构化的知识库来回答问题；
                                 社区问答（Community QA），依赖于用户生成的问答数据，如问答社区，论坛等。\\
\hline
\hline  
\caption{LLM 任务类别}
\end{longtable}


\section{文本表示}
\begin{itemize}
    \item 词向量： Vector Space Model (VSM). 
    
    向量空间模型通过将文本（包括单词，句子，段落或整个文档）转换为高维空间中的向量来实现文本的数学化表示。
    
    在这个模型中，每个维度代表一个特征项（例如字，词，词语或短语），而向量中每个元素值代表该特征项在文本中的权重。这种权重通过特定的计算公式（如词频TF， 逆文档频率TF-IDF）来确定，反映了特征项在文本中的重要程度。

    VSM应用及其广泛，包括但不限于文本相似度计算，文本分类，信息检索等。 它将复杂的文本数据转换为易于计算和分析的数学形式。

    VSM也存在很多问题。其中最主要的是数据稀疏性和维数灾难，因为特征数量庞大导致向量维度极高，同时多数元素值为零。
    此外，由于模型基于特征项之间的独立性假设，忽略了文本中的结构信息（如词序和上下文信息），限制了模型了表现力。
    特征项的选择和权重计算方法的不足也是VSM需要解决的问题。

    为了解决这些问题， 对VSM的研究主要集中在两方面： 1.改进特征表示方法，如借助图方法，主题方法等关键词抽取。 2. 改进和优化特征项权重的计算方法，可以在现有的方法基础上进行融合计算或提出新的计算方法。

    VSM例子：

    “雍和宫的荷花很美”
    
    词汇表大小：16384. 句子包含词汇：[“雍和宫”， “的”， “荷花”， “很”， “美”] 共5个词。

    vector = [0, 0, ... 1, 0, ...1,0, ... 1, 0, ...1, 0, ..., 1, 0, ...]

    16384维的vector中有5个位置为1，其余为0 （16384-5 = 16379）
    
    实际有效维度： 5

    稀疏率： 16379 / 16384 ~= 99.97\%


    \item N-gram
    
    N-gram 是一种基于统计的语言模型，应用于语音识别，手写识别，拼写纠错，机器翻译和搜索引擎等。

    N-gram的核心思想基于马尔科夫假设，即一个词的出现概率仅依赖于它前面的N-1个词。这里的N代表连续出现单词的数量，可以是任意正整数。

    当 N=1时称为 unigram，仅考虑单个词的概率。 

    当 N=2时称为bigram，考虑前一个词来估计当前词的概率。

    当 N=3时称为 trigram，考虑前两个词来估计第三个词的概率，以此类推N-gram。

    N-gram通过条件概率链式规则来估计整个句子的概率。具体而言，对于给定的句子，模型会计算每个N-gram出现的条件概率，并将这些概率相乘得到整个句子的概率。

    优点：实现简单，容易理解。 缺点：当N较大时，会出现数据稀疏性的问题。模型的参数空间急剧增大，相同的N-gram序列出现的概率变得非常低，导致模型无法有效学习，泛化能力下降。
    此外，N-gram忽略了词之间的范围依赖关系，无法捕捉到句子中的复杂结构和语义信息。

    N-gram例子：

    句子："The quick brown fox", 做trigram模型。

    需要计算 $P("brown" | "The", "quick") $, $P("fox" | "quick", "brown") $ 的概率，并相乘。


    \item Word2vec
    
    是一种基于神经网络NNLM的语言模型，通过学习词与词之间的上下文关系来生成词的密集向量表示。核心思想是利用词在文本中的上下文信息来捕捉词之间的语义关系，
    从而使得语义相似或相关的词在向量空间中距离较近。

    主要两种架构：

    连续词袋模型 CBOW （Continuous Bad of Words). 根据目标词上下文中的词对应的词向量，计算并输出目标词的向量表示。适用于小型数据集。

    Skip-Gram： 利用目标词的向量表示计算上下文中的词向量。 适用大型语料。

    相比较传统的高维稀疏表示（like one-hot encoding), Word2vec生成低维密集向量（通常几百维），有助于减少计算复杂度和存储需求。

    Word2vec能捕捉到词与词之间的语义关系，比如“国王”和“王后”在向量空间中的位置比较接近。
    但由于CBOW/Skip-gram是基于局部上下文的，无法捕捉到长距离的依赖关系，缺乏整体的词与词之间的关系，因此在一些复杂的语义任务上表现不佳。


    \item ELMo Embeddings from Language Models
    
    该模型实现了一词多义，静态词向量到动态词向量的跨越式转变。

    首先在大型语料库上训练语言模型，得到词向量模型，然后在特定任务上对模型进行微调，得到更适合该任务的词向量。
     ELMo首次将预训练思想引入到词向量的生成中，适用双向LSTM结构，能够捕捉到词汇的上下文信息，生成更加丰富和准确的词向量表示。

     第一阶段是利用语言模型进行预训练。第二阶段是在做特定任务时，从预训练网络中提取对应单词的词向量作为新特征补充到下游任务重。基于RNN的LSTM模型训练时间长，特征提取是ELMo模型优化和提升的关键。

     ELMo的主要问题是模型复杂度高，训练时间长，计算资源消耗大。

\end{itemize}



\section{Attention Mechanism 注意力机制}

注意力机制三个核心变量: Query(查询值), Key（键值）, Value（真值）
\begin{equation}
    attention(Q,K,V) = softmax(\frac{QK^T}{\sqrt{d_k}})V
    \label{eq:attention}
\end{equation}

Q is the Query matrix, K is the Key matrix, V is the Value matrix, and $d_k$ is the dimension of the key vectors.
Usually Q and K are of the same dimension $d_k$

注意力机制的本质是对两段序列的元素依次进行相似度的计算，寻找出一个序列的每个元素对另一个序列的每个元素的相关度，然后基于相关度进行加权，即分配注意力。
而这两段序列即是我们计算过程中QKV的来源。

但是在实际应用中，我们往往只需要计算Query和Key之间的注意力结果，很少存在额外的真值Value。
在经典注意力机制中，Q往往来自于一个序列，K与V来自另一个序列，都通过参数矩阵计算得到，从而可以你和这两个序列之间的关系。
例如在Transformer的Decoder结构中，Q来自于Decoder的输入，K与V来自于Encoder的输出，从而拟合了编码信息与历史信息之间的关系，
实现了Decoder对Encoder输出的关注。

但在Transformer的Encoder结构中，使用的是自注意力机制（self-attention)。
计算本身序列中的每个元素对其他元素的注意力分布，即在计算过程中，QKV都由同一个输入通过不同的参数矩阵计算得到。
在Encoder中QKV分别是输入对参数矩阵$W_q$, $W_k$, $W_v$ 做积得到，从而拟合输入语句中每一个token对其他的所有token的关系。

通过自注意力机制，我们可以找到一段文本中每一个token与其他所有token的相关关系大小，从而建模文本之间的依赖关系。
在代码中的实现，self-attention是通过QKV的输入传入同一个参数实现:

$attention(X,X,X)$


\section{Mask Self-Attention 掩码自注意力机制}

% \bibliography{reference}
\end{document}